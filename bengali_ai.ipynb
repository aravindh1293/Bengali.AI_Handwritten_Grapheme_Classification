{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bengali-ai.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN64yHcPQN1uc1LYoOSPlNr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2eab0cf606d344d39cec4f5f3035b33b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e84f50b20f1840d6be7ea629798fee12",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e90cd09401ec49f292feeb50d753e6cc",
              "IPY_MODEL_1804535b076e4ac4b2c170dc6159c497"
            ]
          }
        },
        "e84f50b20f1840d6be7ea629798fee12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e90cd09401ec49f292feeb50d753e6cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_169b9d5aaa314b1eab6a340b21a9088f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 87306240,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 87306240,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f6fa47ddcc840c498932ae60d11cece"
          }
        },
        "1804535b076e4ac4b2c170dc6159c497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3a7cb10178f2459ea2bc082a0783b7d4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 83.3M/83.3M [00:02&lt;00:00, 29.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8040e8166af4b758398939e8e3f9a04"
          }
        },
        "169b9d5aaa314b1eab6a340b21a9088f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f6fa47ddcc840c498932ae60d11cece": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a7cb10178f2459ea2bc082a0783b7d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8040e8166af4b758398939e8e3f9a04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ5s_v8NsWl8",
        "colab_type": "code",
        "outputId": "580269ba-928c-4afe-cf7e-69c4f19b7e9b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json\n",
        "\n",
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5830174f-6700-4d14-834e-473eb86370b9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5830174f-6700-4d14-834e-473eb86370b9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 67 Mar  8 07:44 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-ALRssl0IOO",
        "colab_type": "code",
        "outputId": "e6c66953-a963-4697-8449-37ecf3ee55f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!kaggle competitions download -c bengaliai-cv19\n",
        "!mkdir zip_files\n",
        "!mv ./* zip_files\n",
        "!unzip 'zip_files/*.zip' -d input\n",
        "!cp zip_files/*.csv input/\n",
        "!rm -rf zip_files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading class_map_corrected.csv to /content\n",
            "  0% 0.00/4.75k [00:00<?, ?B/s]\n",
            "100% 4.75k/4.75k [00:00<00:00, 8.61MB/s]\n",
            "Downloading test_image_data_1.parquet.zip to /content\n",
            "  0% 0.00/1.30M [00:00<?, ?B/s]\n",
            "100% 1.30M/1.30M [00:00<00:00, 88.1MB/s]\n",
            "Downloading train_image_data_2.parquet.zip to /content\n",
            " 98% 973M/992M [00:03<00:00, 258MB/s]\n",
            "100% 992M/992M [00:04<00:00, 255MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            "  0% 0.00/1.32M [00:00<?, ?B/s]\n",
            "100% 1.32M/1.32M [00:00<00:00, 191MB/s]\n",
            "Downloading train_multi_diacritics.csv to /content\n",
            "  0% 0.00/16.8k [00:00<?, ?B/s]\n",
            "100% 16.8k/16.8k [00:00<00:00, 17.4MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/944 [00:00<?, ?B/s]\n",
            "100% 944/944 [00:00<00:00, 748kB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/1.70k [00:00<?, ?B/s]\n",
            "100% 1.70k/1.70k [00:00<00:00, 1.91MB/s]\n",
            "Downloading test_image_data_3.parquet.zip to /content\n",
            "  0% 0.00/1.28M [00:00<?, ?B/s]\n",
            "100% 1.28M/1.28M [00:00<00:00, 182MB/s]\n",
            "Downloading test_image_data_0.parquet.zip to /content\n",
            "  0% 0.00/1.18M [00:00<?, ?B/s]\n",
            "100% 1.18M/1.18M [00:00<00:00, 172MB/s]\n",
            "Downloading class_map.csv to /content\n",
            "  0% 0.00/4.72k [00:00<?, ?B/s]\n",
            "100% 4.72k/4.72k [00:00<00:00, 3.17MB/s]\n",
            "Downloading test_image_data_2.parquet.zip to /content\n",
            "  0% 0.00/1.25M [00:00<?, ?B/s]\n",
            "100% 1.25M/1.25M [00:00<00:00, 177MB/s]\n",
            "Downloading train_image_data_1.parquet.zip to /content\n",
            " 99% 977M/986M [00:06<00:00, 60.2MB/s]\n",
            "100% 986M/986M [00:06<00:00, 167MB/s] \n",
            "Downloading train_image_data_0.parquet.zip to /content\n",
            " 98% 971M/991M [00:28<00:00, 55.5MB/s]\n",
            "100% 991M/991M [00:28<00:00, 36.4MB/s]\n",
            "Downloading train_image_data_3.parquet.zip to /content\n",
            " 99% 981M/993M [00:18<00:00, 159MB/s]\n",
            "100% 993M/993M [00:18<00:00, 56.0MB/s]\n",
            "mv: cannot move './zip_files' to a subdirectory of itself, 'zip_files/zip_files'\n",
            "Archive:  zip_files/train_image_data_0.parquet.zip\n",
            "  inflating: input/train_image_data_0.parquet  \n",
            "\n",
            "Archive:  zip_files/train.csv.zip\n",
            "  inflating: input/train.csv         \n",
            "\n",
            "Archive:  zip_files/test_image_data_3.parquet.zip\n",
            "  inflating: input/test_image_data_3.parquet  \n",
            "\n",
            "Archive:  zip_files/train_image_data_1.parquet.zip\n",
            "  inflating: input/train_image_data_1.parquet  \n",
            "\n",
            "Archive:  zip_files/test_image_data_0.parquet.zip\n",
            "  inflating: input/test_image_data_0.parquet  \n",
            "\n",
            "Archive:  zip_files/train_image_data_2.parquet.zip\n",
            "  inflating: input/train_image_data_2.parquet  \n",
            "\n",
            "Archive:  zip_files/test_image_data_1.parquet.zip\n",
            "  inflating: input/test_image_data_1.parquet  \n",
            "\n",
            "Archive:  zip_files/test_image_data_2.parquet.zip\n",
            "  inflating: input/test_image_data_2.parquet  \n",
            "\n",
            "Archive:  zip_files/train_image_data_3.parquet.zip\n",
            "  inflating: input/train_image_data_3.parquet  \n",
            "\n",
            "9 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpa7nlRB_yfL",
        "colab_type": "code",
        "outputId": "80bf8c9e-51e3-458c-cf93-9f87a9932e81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "!pip install iterative-stratification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.17.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (0.14.1)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90_e3Pcf8c9Q",
        "colab_type": "code",
        "outputId": "c67eb088-4977-4460-ee11-b50377721773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "!pip install pretrainedmodels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pretrainedmodels\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\r\u001b[K     |█████▋                          | 10kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.5.0)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.28.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (6.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (1.17.5)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60962 sha256=5c636379393d6a42d7c26e9bbe14feb658c16741ef538ccb5a086616a908e729\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HKDNxTn_rGT",
        "colab_type": "text"
      },
      "source": [
        "# create folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjddgYzO36et",
        "colab_type": "code",
        "outputId": "129ef01e-94eb-4064-d2d4-7a4908909972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "import pandas as pd\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv(\"input/train.csv\")\n",
        "    df['kfold'] = -1\n",
        "    # print(df)\n",
        "\n",
        "    #shuffle the dataset and also reset its index\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    x = df.image_id.values\n",
        "    y = df[[\"grapheme_root\", \"vowel_diacritic\", \"consonant_diacritic\"]].values\n",
        "\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=5)\n",
        "\n",
        "    for fold, (trn_, val_) in enumerate(mskf.split(x,y)):\n",
        "        # print(\"val : \", len(val_))\n",
        "        print(f\"Train :{trn_}, val : {val_}\")\n",
        "        df.loc[val_, \"kfold\"] = fold\n",
        "\n",
        "    print(\"kfolds : \",df.kfold.values)\n",
        "    print(\"counts : \",df.kfold.value_counts())\n",
        "    df.to_csv(\"input/train_folds.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train :[     0      1      2 ... 200837 200838 200839], val : [     5      6      8 ... 200825 200832 200835]\n",
            "Train :[     0      1      2 ... 200836 200837 200839], val : [     3      9     12 ... 200821 200828 200838]\n",
            "Train :[     0      1      3 ... 200837 200838 200839], val : [     2     18     22 ... 200830 200833 200834]\n",
            "Train :[     0      1      2 ... 200836 200837 200838], val : [     4     15     24 ... 200826 200831 200839]\n",
            "Train :[     2      3      4 ... 200835 200838 200839], val : [     0      1      7 ... 200829 200836 200837]\n",
            "kfolds :  [4 4 2 ... 4 1 3]\n",
            "counts :  4    40168\n",
            "3    40168\n",
            "2    40168\n",
            "1    40168\n",
            "0    40168\n",
            "Name: kfold, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GJOWxt9EyO5",
        "colab_type": "text"
      },
      "source": [
        "### COnvert images to pickes for faster computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zf6CeOz_su7",
        "colab_type": "code",
        "outputId": "67257da0-da60-47be-e53c-b7f92f0b8d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    files = glob.glob(\"input/train*.parquet\")\n",
        "    print(files)\n",
        "    os.mkdir(\"input/image_pickles\")\n",
        "    for f in files:\n",
        "        df = pd.read_parquet(f)\n",
        "        image_ids = df.image_id.values\n",
        "        df = df.drop(\"image_id\", axis=1)\n",
        "        image_array = df.values\n",
        "        for j, image_id in tqdm(enumerate(image_ids), total=len(image_ids)):\n",
        "            joblib.dump(image_array[j], f\"input/image_pickles/{image_id}.pkl\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['input/train_image_data_3.parquet', 'input/train_image_data_1.parquet', 'input/train_image_data_2.parquet', 'input/train_image_data_0.parquet']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 50210/50210 [00:29<00:00, 1678.07it/s]\n",
            "100%|██████████| 50210/50210 [00:32<00:00, 1528.62it/s]\n",
            "100%|██████████| 50210/50210 [00:30<00:00, 1639.64it/s]\n",
            "100%|██████████| 50210/50210 [00:46<00:00, 1080.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L6jYBICE4xC",
        "colab_type": "text"
      },
      "source": [
        "# dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2eQAbaeAEUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import albumentations\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class BengaliDatasetTrain:\n",
        "    def __init__(self, folds, img_height, img_width, mean, std):\n",
        "        df = pd.read_csv(\"input/train_folds.csv\")\n",
        "        df = df[[\"image_id\", \"grapheme_root\", \"vowel_diacritic\", \"consonant_diacritic\", \"kfold\"]]\n",
        "\n",
        "        df = df[df.kfold.isin(folds)].reset_index(drop=True)\n",
        "        self.image_ids = df.image_id.values\n",
        "        self.grapheme_root = df.grapheme_root.values\n",
        "        self.vowel_diacritic = df.vowel_diacritic.values\n",
        "        self.consonant_diacritic = df.consonant_diacritic.values\n",
        "        \n",
        "        # apply augmentation if we are on validation phase\n",
        "        if len(folds) == 1:\n",
        "            self.aug = albumentations.Compose([\n",
        "                                            albumentations.Resize(img_height, img_width, always_apply=True),\n",
        "                                            albumentations.Normalize(mean, std, always_apply=True)\n",
        "            ]) \n",
        "        else : \n",
        "            self.aug = albumentations.Compose([\n",
        "                                            albumentations.Resize(img_height, img_width, always_apply=True),\n",
        "                                            albumentations.Normalize(mean, std, always_apply=True),\n",
        "                                            albumentations.ShiftScaleRotate(shift_limit=0.0625, \n",
        "                                                                            scale_limit=0.1, \n",
        "                                                                            rotate_limit=5,\n",
        "                                                                            p=0.9)\n",
        "            ]) \n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = joblib.load(f\"input/image_pickles/{self.image_ids[idx]}.pkl\")\n",
        "        image = image.reshape(137, 236).astype(float)\n",
        "        image = Image.fromarray(image).convert('RGB')\n",
        "        image = self.aug(image=np.array(image))[\"image\"]\n",
        "\n",
        "        image = np.transpose(image, (2,0,1)).astype(np.float32)\n",
        "        return {\n",
        "            \"image\" : torch.tensor(image, dtype=torch.float),\n",
        "            \"grapheme_root\" : torch.tensor(self.grapheme_root[idx], dtype=torch.long),\n",
        "            \"vowel_diacritic\" : torch.tensor(self.vowel_diacritic[idx], dtype=torch.long),\n",
        "            \"consonant_diacritic\" : torch.tensor(self.consonant_diacritic[idx], dtype=torch.long)\n",
        "        }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvcS2N-UTv31",
        "colab_type": "text"
      },
      "source": [
        "### visualising data in dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTFQHhMxL--l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dQfYOZYMZBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = BengaliDatasetTrain(folds=[0,1], \n",
        "                           img_height=137, \n",
        "                           img_width=236, \n",
        "                           mean=(0.485, 0.456, 0.406), \n",
        "                           std=(0.229, 0.224, 0.225))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gRiG2LvM0So",
        "colab_type": "code",
        "outputId": "3b4d8596-e263-43b5-dee9-0a8a7c9d4a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80336"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHpQ61blNW_E",
        "colab_type": "code",
        "outputId": "a966f5f3-e1a0-46c4-9b79-e6aa3457a814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "idx = 60000\n",
        "img = data[idx]\n",
        "plt.imshow(np.transpose(img['image'].numpy(), (1,2,0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f86265d1f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAADlCAYAAACoGbcCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0Y0qy-S7_8v",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxtZPM00OTIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pretrainedmodels\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TkJEUag9CZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet34(nn.Module):\n",
        "    def __init__(self, pretrained):\n",
        "        super(ResNet34, self).__init__()\n",
        "        if pretrained:  \n",
        "            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=\"imagenet\")\n",
        "        else:\n",
        "            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=None)\n",
        "\n",
        "        #168 grapheme\n",
        "        self.l0 = nn.Linear(512, 168)\n",
        "        # 11 vowel\n",
        "        self.l1 = nn.Linear(512, 11)\n",
        "        # 7 consonant\n",
        "        self.l2 = nn.Linear(512, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, _, _,_ = x.shape\n",
        "        x = self.model.features(x)\n",
        "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
        "        l0 = self.l0(x)\n",
        "        l1 = self.l1(x)\n",
        "        l2 = self.l2(x)\n",
        "        return l0, l1, l2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzyN5DKoTsQb",
        "colab_type": "code",
        "outputId": "71c0e59a-2f87-4053-b590-4579e63886fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "2eab0cf606d344d39cec4f5f3035b33b",
            "e84f50b20f1840d6be7ea629798fee12",
            "e90cd09401ec49f292feeb50d753e6cc",
            "1804535b076e4ac4b2c170dc6159c497",
            "169b9d5aaa314b1eab6a340b21a9088f",
            "3f6fa47ddcc840c498932ae60d11cece",
            "3a7cb10178f2459ea2bc082a0783b7d4",
            "d8040e8166af4b758398939e8e3f9a04"
          ]
        }
      },
      "source": [
        "model = ResNet34(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2eab0cf606d344d39cec4f5f3035b33b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=87306240), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y6-Hf8IBkXM",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmiPG_ZHBjiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# import ast\n",
        "\n",
        "# DEVICE = \"cuda\"\n",
        "# TRAINING_FOLDS_CSV = os.environ.get(\"TRAINING_FOLDS_CSV\")\n",
        "# IMG_HEIGHT = int(os.environ.get(\"IMG_HEIGHT\"))\n",
        "# IMG_WIDTH = int(os.environ.get(\"IMG_WIDTH\"))\n",
        "# EPOCHS = int(os.environ.get(\"EPOCHS\"))\n",
        "\n",
        "# TRAIN_BATCH_SIZE = int(os.environ.get(\"TRAIN_BATCH_SIZE\"))\n",
        "# TEST_BATCH_SIZE = int(os.environ.get(\"TEST_BATCH_SIZE\"))\n",
        "\n",
        "# IMAGE_MEAN = ast.literal_eval(os.environ.get(\"MODEL_MEAN\"))\n",
        "# IMAGE_STD = ast.literal_eval(os.environ.get(\"MODEL_STD\"))\n",
        "\n",
        "# TRAINING_FOLDS = int(os.environ.get(\"TRAINING_FOLDS\"))\n",
        "# VALIDATION_FOLDS = int(os.environ.get(\"VALIDATION_FOLDS\"))\n",
        "# BASE_MODEL = os.environ.get(\"BASE_MODEL\")\n",
        "\n",
        "import os\n",
        "import ast\n",
        "\n",
        "DEVICE = \"cuda\"\n",
        "TRAINING_FOLDS_CSV = \"input/train_folds.csv\"\n",
        "IMG_HEIGHT = 137\n",
        "IMG_WIDTH = 236\n",
        "EPOCHS = 5\n",
        "\n",
        "TRAIN_BATCH_SIZE = 356\n",
        "TEST_BATCH_SIZE = 224\n",
        "\n",
        "IMAGE_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGE_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "TRAINING_FOLDS = (0,1,2,3)\n",
        "VALIDATION_FOLDS = (4,)\n",
        "BASE_MODEL = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5gAAi97E9Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLknAodyDjqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train\n",
        "\n",
        "def main():\n",
        "\n",
        "    model = ResNet34(pretrained=True)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    train_dataset = BengaliDatasetTrain(folds=TRAINING_FOLDS,\n",
        "                                        img_height = IMG_HEIGHT,\n",
        "                                        img_width = IMG_WIDTH,\n",
        "                                        mean = IMAGE_MEAN,\n",
        "                                        std = IMAGE_STD)\n",
        "     \n",
        "    #train dataloader\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size= TRAIN_BATCH_SIZE,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=4)\n",
        "    \n",
        "    valid_dataset = BengaliDatasetTrain(folds=VALIDATION_FOLDS,\n",
        "                                        img_height = IMG_HEIGHT,\n",
        "                                        img_width = IMG_WIDTH,\n",
        "                                        mean = IMAGE_MEAN,\n",
        "                                        std=IMAGE_STD)\n",
        "    \n",
        "    #validation dataloader\n",
        "    val_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                             batch_size= TEST_BATCH_SIZE,\n",
        "                                             shuffle=False,\n",
        "                                             num_workers=4)\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, \n",
        "                                                           mode=\"min\", \n",
        "                                                           patience=5, \n",
        "                                                           factor=0.3, \n",
        "                                                           verbose=True)\n",
        "\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(\"training............\")\n",
        "        train(train_dataset, train_loader, model, optimizer)\n",
        "        print(\"validating............\")\n",
        "        val_score = evaluate(valid_dataset, val_loader, model)\n",
        "        scheduler.step(val_score)\n",
        "        torch.save(model.state_dict(), f\"resnet_34_fold{VALIDATION_FOLDS[0]}.bin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4toF52n0JoAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, dataloader, model, optimizer):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    counter = 0\n",
        "    for bi, d in tqdm(enumerate(dataloader), total=len(dataset)/dataloader.batch_size):\n",
        "        counter = counter+1\n",
        "        image = d['image'].to(DEVICE, dtype=torch.float)\n",
        "        grapheme_root = d['grapheme_root'].to(DEVICE, dtype=torch.long)\n",
        "        vowel_diacritic = d['vowel_diacritic'].to(DEVICE, dtype=torch.long)\n",
        "        consonant_diacritic = d['consonant_diacritic'].to(DEVICE, dtype=torch.long)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(image)\n",
        "        targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        final_loss += loss\n",
        "    print(\"train loss : \", final_loss / counter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo7eLVpPN7bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(dataset, dataloader, model):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    counter = 0\n",
        "    with torch.no_grad():\n",
        "        for bi, d in tqdm(enumerate(dataloader), total=len(dataset)/dataloader.batch_size):\n",
        "            counter = counter+1\n",
        "            image = d['image'].to(DEVICE, dtype=torch.float)\n",
        "            grapheme_root = d['grapheme_root'].to(DEVICE, dtype=torch.long)\n",
        "            vowel_diacritic = d['vowel_diacritic'].to(DEVICE, dtype=torch.long)\n",
        "            consonant_diacritic = d['consonant_diacritic'].to(DEVICE, dtype=torch.long)\n",
        "            outputs = model(image)\n",
        "            targets = (grapheme_root, vowel_diacritic, consonant_diacritic)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            final_loss += loss\n",
        "        print(\"test loss : \", final_loss / counter)\n",
        "\n",
        "    return final_loss / counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzPiq5WBOhqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    o1, o2, o3 = outputs\n",
        "    t1, t2, t3 = targets\n",
        "\n",
        "    l1 = nn.CrossEntropyLoss()(o1, t1)\n",
        "    l2 = nn.CrossEntropyLoss()(o1, t2)\n",
        "    l3 = nn.CrossEntropyLoss()(o1, t3)\n",
        "    return (l1+l2+l3) / 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWuqpXWbZenb",
        "colab_type": "code",
        "outputId": "fe9037a1-73ce-4cd6-a80c-3ca519e7551a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "452it [24:06,  2.66s/it]                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss :  tensor(1.8402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "validating............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "180it [01:41,  1.78it/s]                                        \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test loss :  tensor(1.2979, device='cuda:0')\n",
            "training............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "452it [24:08,  2.67s/it]                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss :  tensor(1.2283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "validating............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "180it [01:41,  1.79it/s]                                        \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test loss :  tensor(1.1945, device='cuda:0')\n",
            "training............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "452it [24:08,  2.67s/it]                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss :  tensor(1.1615, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "validating............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "180it [01:41,  1.78it/s]                                        \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test loss :  tensor(1.1705, device='cuda:0')\n",
            "training............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "452it [24:08,  2.67s/it]                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss :  tensor(1.1329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "validating............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "180it [01:41,  1.78it/s]                                        \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test loss :  tensor(1.1469, device='cuda:0')\n",
            "training............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "452it [24:08,  2.67s/it]                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss :  tensor(1.1125, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "validating............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "180it [01:41,  1.78it/s]                                        \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test loss :  tensor(1.1379, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbIzVXjkFo0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}